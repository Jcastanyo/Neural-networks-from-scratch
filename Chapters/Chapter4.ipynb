{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4 - Activation Functions"
      ],
      "metadata": {
        "id": "OW3d1Z9VqWi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we use activation functions?\n",
        "\"We use activation functions because if\n",
        "the activation function itself is nonlinear, it allows for neural networks with usually two or more\n",
        "hidden layers to map nonlinear functions.\", extracted from the original book."
      ],
      "metadata": {
        "id": "fh2mbRmUqniO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReLU activation function class"
      ],
      "metadata": {
        "id": "Lg0viJfZ3m4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaJeEo1F4sOw",
        "outputId": "acd0e17b-2b47-453d-a606-6745b6ba674d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.21.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nnfs.datasets import spiral_data\n",
        "import nnfs"
      ],
      "metadata": {
        "id": "wrFRW5NT39ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUhU_I7zqPxB"
      },
      "outputs": [],
      "source": [
        "# ReLU activation class\n",
        "class Activation_ReLU:\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from input\n",
        "    self.output = np.maximum(0, inputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax activation class\n",
        "class Activation_Softmax:\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    \n",
        "    # Get unnormalized probabilities\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "\n",
        "    # Normalize them for each sample\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "    self.output = probabilities"
      ],
      "metadata": {
        "id": "VDCU0WNHCdJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# As we explained in the previous chapter, there are dead neurons, but there also are exploding neurons. These neurons outputs very large values. Very large values can make the network useless over the training time. Besides, in an exponential function, the bigger the input, the bigger the output. "
      ],
      "metadata": {
        "id": "c13I2TfdDQUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# examples of exploding neurons and very large values\n",
        "\n",
        "# here we see that bigger the input, bigger the output\n",
        "print(np.exp(1))\n",
        "print(np.exp(10))\n",
        "print(np.exp(100))\n",
        "#print(np.exp(1000)) # returns infinite, overflow error\n",
        "\n",
        "# we know that exp(-inf) tends to 0, and exp(0) is 1\n",
        "print(np.exp(-np.inf), np.exp(0))\n",
        "\n",
        "# we can use this trick to subtract the max value to each value. First, we subtract\n",
        "# the max value to each value in output vector. Then, we have some negative values\n",
        "# and the max value that now is 0. Applying softmax we eliminate the negative sign\n",
        "# because the exp values are negative and the sum in the denominator is also negative.\n",
        "# Then, the final division is always positive. \n",
        "\n",
        "# THE SUBTRACTION HELPS TO REDUCE THE MAGNITUD OF THE NEURON OUTPUT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHEi3YzeD23c",
        "outputId": "ea800b3d-74e7-45cb-ffa5-c569bd3b1934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.718281828459045\n",
            "22026.465794806718\n",
            "2.6881171418161356e+43\n",
            "0.0 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer_Dense:\n",
        "\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    \"\"\"\n",
        "    Initialise weights and biases\n",
        "    random.randn returns a matrix with n_inputs x n_neurons shape\n",
        "    it makes sense to create the weights matrix with inputs x neurons shape\n",
        "    NUMBER OF WEIGHTS VALUES = NUMBER OF INPUTS X NUMBER OF NEURONS\n",
        "    \n",
        "    note that we define the shape as inputs X neurons and not neurons X inputs\n",
        "    to avoid transposing every time we do a forward pass\n",
        "    \n",
        "        4 inputs, 3 neurons\n",
        "        wij -> weight, i -> input value, j -> neuron \n",
        "        [w11, w12, w13]\n",
        "        [w21, w22, w23]\n",
        "        [w31, w32, w33]\n",
        "        [w41, w42, w43]\n",
        "\n",
        "        each column is already a vector of weights of each neuron for the input\n",
        "    \n",
        "    random.randn generates values from a Gaussian distribution with a mean of 0\n",
        "    and a variance of 1, which mean that it'll generate random numbers, positive and\n",
        "    negative [-1,1], centered at 0 and with the mean value close to 0.\n",
        "    \n",
        "    we multiply here by 0.01 because we want to initialise weights with non-zero values\n",
        "    but these values have to be small because training updates will be smaller. \n",
        "    If weight values are very big, the training will last more time.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    # ONE BIAS VALUE FOR EACH NEURON\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Calculate outputs values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "o9LQ3_Bf5KHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nnfs.init()\n",
        "# create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)  # samples per class\n",
        "\n",
        "# -----------------model------------\n",
        "\n",
        "# create dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "\n",
        "# Create ReLU activation (to beb used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "\n",
        "# Create softmax activation (to be used with dense layer)\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "# ----------------forward------------\n",
        "\n",
        "# Perform a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Forward pass through activation func.\n",
        "# Takes in output from previous layer\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "# Make a forward pass through second dense layer\n",
        "# it takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "# in softmax, the number of inputs is the number of final classes\n",
        "# here we have 3 classes\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "\n",
        "print(activation2.output[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqj7Xetk4ybq",
        "outputId": "e180a8aa-0648-4834-f87f-f0543e2f110b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.3333332  0.3333332  0.33333364]\n",
            " [0.3333329  0.33333293 0.3333342 ]\n",
            " [0.3333326  0.33333263 0.33333477]\n",
            " [0.33333233 0.3333324  0.33333528]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sum, axis and keepdims"
      ],
      "metadata": {
        "id": "y8fHwKYt-0iR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "layer_outputs = np.array([[4.8, 1.21, 2.385],\n",
        "                          [8.9, -1.81, 0.2],\n",
        "                          [1.41, 1.051, 0.026]])\n",
        "\n",
        "# same value in both operations because default axis is None\n",
        "# here, as we do not indicate any axis, sums element by element\n",
        "print(np.sum(layer_outputs))\n",
        "print(np.sum(layer_outputs, axis=None))\n",
        "\n",
        "# with axis=0, here we sum [4.8, 1.21, 2.385] + [8.9, 1.81, 0.2] + [1.41, 1.051, 0.026] = \n",
        "# = [15.11, 0.451, 2.611]\n",
        "print(np.sum(layer_outputs, axis=0))\n",
        "\n",
        "# with axis=1, we sum the columns inside of each row\n",
        "print(np.sum(layer_outputs, axis=1))\n",
        "print(np.sum(layer_outputs, axis=1).shape)\n",
        "\n",
        "# later we need to divide the exp vector by the sum, then we need to keep \n",
        "# the layer ouputs shape\n",
        "print(np.sum(layer_outputs, axis=1, keepdims=True))\n",
        "print(np.sum(layer_outputs, axis=1, keepdims=True).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za26vIAY-3Ky",
        "outputId": "f1923e0e-c6c9-4aed-a1e4-0f0f3151043c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18.172\n",
            "18.172\n",
            "[15.11   0.451  2.611]\n",
            "[8.395 7.29  2.487]\n",
            "(3,)\n",
            "[[8.395]\n",
            " [7.29 ]\n",
            " [2.487]]\n",
            "(3, 1)\n"
          ]
        }
      ]
    }
  ]
}