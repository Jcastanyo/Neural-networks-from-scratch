{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 5 - Calculating Network Error with Loss"
      ],
      "metadata": {
        "id": "MtrRmSjOleZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUI36JavAZ5f",
        "outputId": "f7494620-e34f-4447-ed66-63f6c47c3a74"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.21.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical cross entropy error\n",
        "L = -SUM(y_true * log(y_pred))\n",
        "\n",
        "In fact, it is the same that calculating only -log(softmax_output[ground_truth==1]). Thus, we only do one operation instead of multiplying \n",
        "0 labels by softmax probability."
      ],
      "metadata": {
        "id": "5sjgXRTrmev2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i78iYWckrSe",
        "outputId": "dc1adb60-fc74-4918-e287-bff1a42f0871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35667494393873245 0.35667494393873245 True\n"
          ]
        }
      ],
      "source": [
        "import math \n",
        "\n",
        "# An example output from the output layer of the neural network\n",
        "softmax_output = [0.7, 0.1, 0.2]\n",
        "\n",
        "# Ground Truth\n",
        "target_output = [1, 0, 0]\n",
        "\n",
        "# -(1 * log(0.7) + 0 * ... + 0 * ...)\n",
        "# = - log(0.7)\n",
        "\n",
        "loss = -(math.log(softmax_output[0]) * target_output[0] + \n",
        "         math.log(softmax_output[1]) * target_output[1] + \n",
        "         math.log(softmax_output[2]) * target_output[2])\n",
        "\n",
        "# - log(0.7)\n",
        "loss_reduced = -(math.log(softmax_output[target_output==1]))\n",
        "\n",
        "print(loss, loss_reduced, loss == loss_reduced)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here we see that log values near to 1 tend to zero, but values near to zero are bigger (forget about the minus because the formula multiplies by - to make the value positive). Then, the greater the probability, the smallest the loss."
      ],
      "metadata": {
        "id": "bajRj6pFsZVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(0.05, 1, 19)\n",
        "\n",
        "xx = [math.log(i) for i in x]\n",
        "\n",
        "plt.plot(np.linspace(0, 1, len(xx)),xx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "MtRBBTWvqXyi",
        "outputId": "1218c993-6399-4dbb-993a-6ded90cfba45"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f03a2172350>]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
        "                           [0.1, 0.5, 0.4],\n",
        "                           [0.02, 0.9, 0.08]])\n",
        "class_targets = [0, 1, 1]\n",
        "\n",
        "# we can get the target probabilities of softmax_outputs directly with numpy\n",
        "# we can use a list of indices to get values from a numpy array\n",
        "\n",
        "# for example\n",
        "a = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
        "b = [1, 2, 0]\n",
        "\n",
        "print(a[[0, 1, 2], b])\n",
        "\n",
        "print(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
        "\n",
        "# then, now we can calculate the -log\n",
        "loss = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
        "\n",
        "print(loss)\n",
        "\n",
        "# now, we want to calculate the average loss of the whole batch\n",
        "# to see how our model is performing\n",
        "avg_loss = np.mean(loss)\n",
        "\n",
        "print(avg_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RnDvDnLvOdW",
        "outputId": "c273c7e2-3d50-4930-e49c-fb7a60df3f16"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 6 7]\n",
            "[0.7 0.5 0.9]\n",
            "[0.35667494 0.69314718 0.10536052]\n",
            "0.38506088005216804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Categorical Cross-Entropy Loss Class"
      ],
      "metadata": {
        "id": "HlhaXm-E0Da3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first, we create our global loss class. If we use more than one loss, i.e. for\n",
        "# object detection, the global loss is the mean value between class loss and \n",
        "# box loss.\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "  # Calculates the data and regularizations losses given model output and\n",
        "  # ground truth values\n",
        "  def calculate(self, output, y):\n",
        "\n",
        "    # Calculate sample losses\n",
        "    sample_losses = self.forward(output, y)\n",
        "\n",
        "    # Calculate mean loss\n",
        "    data_loss = np.mean(sample_losses)\n",
        "\n",
        "    # Return loss\n",
        "    return data_loss\n",
        "\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "\n",
        "    # Number of samples in a batch\n",
        "    samples = len(y_pred)\n",
        "\n",
        "    # Clip data to prevent division by 0 when calculating log. We have to avoid\n",
        "    # calculating log(0). Then, we clip all the output values from softmax between\n",
        "    # 1e-7 and 1-1e-7.\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
        "\n",
        "    # Probabilities for target values - only if categorical labels\n",
        "    # this is for sparse labels [0, 2, 5, 1], each value is a label\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "\n",
        "    # for one hot encoded labels, we multiply each softmax output\n",
        "    # by the label\n",
        "    elif len(y_true.shape) == 2:\n",
        "      correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
        "\n",
        "\n",
        "    # Losses\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "\n",
        "    return negative_log_likelihoods"
      ],
      "metadata": {
        "id": "PGjZawqr0X_N"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# example\n",
        "\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "loss = loss_function.calculate(softmax_outputs, np.asarray(class_targets))\n",
        "\n",
        "print(loss)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "yNbIbpfE_IIx",
        "outputId": "42725c06-693d-471f-c7e5-0d27b35d15b2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# example\\n\\nloss_function = Loss_CategoricalCrossentropy()\\nloss = loss_function.calculate(softmax_outputs, np.asarray(class_targets))\\n\\nprint(loss)'"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining everything up to this point"
      ],
      "metadata": {
        "id": "AL46bqNX_0EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer_Dense:\n",
        "\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    \"\"\"\n",
        "    Initialise weights and biases\n",
        "    random.randn returns a matrix with n_inputs x n_neurons shape\n",
        "    it makes sense to create the weights matrix with inputs x neurons shape\n",
        "    NUMBER OF WEIGHTS VALUES = NUMBER OF INPUTS X NUMBER OF NEURONS\n",
        "    \n",
        "    note that we define the shape as inputs X neurons and not neurons X inputs\n",
        "    to avoid transposing every time we do a forward pass\n",
        "    \n",
        "        4 inputs, 3 neurons\n",
        "        wij -> weight, i -> input value, j -> neuron \n",
        "        [w11, w12, w13]\n",
        "        [w21, w22, w23]\n",
        "        [w31, w32, w33]\n",
        "        [w41, w42, w43]\n",
        "\n",
        "        each column is already a vector of weights of each neuron for the input\n",
        "    \n",
        "    random.randn generates values from a Gaussian distribution with a mean of 0\n",
        "    and a variance of 1, which mean that it'll generate random numbers, positive and\n",
        "    negative [-1,1], centered at 0 and with the mean value close to 0.\n",
        "    \n",
        "    we multiply here by 0.01 because we want to initialise weights with non-zero values\n",
        "    but these values have to be small because training updates will be smaller. \n",
        "    If weight values are very big, the training will last more time.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    # ONE BIAS VALUE FOR EACH NEURON\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Calculate outputs values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# ReLU activation class\n",
        "class Activation_ReLU:\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Calculate output values from input\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Softmax activation class\n",
        "class Activation_Softmax:\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    \n",
        "    # Get unnormalized probabilities\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "\n",
        "    # Normalize them for each sample\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "    self.output = probabilities"
      ],
      "metadata": {
        "id": "-_DBp0Jc_zlS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nnfs \n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "# create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)  # samples per class\n",
        "\n",
        "# -----------------model------------\n",
        "\n",
        "# create dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "\n",
        "# Create ReLU activation (to beb used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "\n",
        "# Create softmax activation (to be used with dense layer)\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "# Create loss function \n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "\n",
        "# ----------------forward------------\n",
        "\n",
        "# Perform a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Forward pass through activation func.\n",
        "# Takes in output from previous layer\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "# Make a forward pass through second dense layer\n",
        "# it takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "# in softmax, the number of inputs is the number of final classes\n",
        "# here we have 3 classes\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "# Perform a forward pass through loss function \n",
        "# it takes the output of the softmax layer here and returns loss\n",
        "# y is sparse\n",
        "loss = loss_function.calculate(activation2.output, y)\n",
        "\n",
        "print(\"loss: \", loss)\n",
        "\n",
        "# Accuracy calculation\n",
        "predictions = np.argmax(activation2.output, axis=1)\n",
        "\n",
        "# if labels are one-hot encoded, convert them to sparse\n",
        "if len(y.shape) == 2:\n",
        "  y = np.argmax(y, axis=1)\n",
        "\n",
        "accuracy = np.mean(predictions==y)\n",
        "\n",
        "print(\"accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrTjVkGdARNu",
        "outputId": "1463972c-e0bc-455a-8e32-03f8227b7335"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  1.0986104\n",
            "accuracy:  0.34\n"
          ]
        }
      ]
    }
  ]
}