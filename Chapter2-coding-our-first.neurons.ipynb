{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2 - Coding Our First Neurons\n",
        "\n"
      ],
      "metadata": {
        "id": "XCmkyATefD2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9QQgHmnKgX4a"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ONE NEURON"
      ],
      "metadata": {
        "id": "gFY9TDKMhnnY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRj6r7Sfeqwd",
        "outputId": "03bef758-ea39-4776-c562-553fd5942335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3\n"
          ]
        }
      ],
      "source": [
        "# one neuron with three inputs\n",
        "# making up inputs and weights\n",
        "inputs  = [1.0, 2.0, 3]\n",
        "weights = [0.2, 0.8, -0.5]\n",
        "bias = 2 # only one value for neuron\n",
        "\n",
        "# output\n",
        "output = (inputs[0] * weights[0] + inputs[1] * weights[1] + \\\n",
        "          inputs[2] * weights[2] + bias)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now, one neuron with four inputs\n",
        "inputs  = [1.0, 2.0, 3.0, 2.5]\n",
        "weights = [0.2, 0.8, -0.5, 1.0]\n",
        "bias = 2 # only one value for neuron\n",
        "\n",
        "# output\n",
        "output = (inputs[0] * weights[0] + inputs[1] * weights[1] + \\\n",
        "          inputs[2] * weights[2] + inputs[3] * weights[3] + bias)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAlCxBP1hXAm",
        "outputId": "cc725b68-2df6-482a-cafc-2fef0d0edcd4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LAYER OF NEURONS"
      ],
      "metadata": {
        "id": "KpfsnLyEhxX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOU SHOULDN'T USE AUX AS VARIABLE NAMES\n",
        "# my implementation using only python, not even numpy\n",
        "def one_layer_mine(inputs, weights, bias):\n",
        "  outputs = []\n",
        "  aux = 0\n",
        "\n",
        "  for b, w_neuron in zip(bias, weights):\n",
        "    aux = 0\n",
        "    for inp, w_input in zip(inputs, w_neuron):\n",
        "      aux += inp * w_input\n",
        "\n",
        "    aux += b\n",
        "\n",
        "    outputs.append(aux)\n",
        "\n",
        "  return outputs"
      ],
      "metadata": {
        "id": "kU4FVRl1pbsQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# they did the same implementation as me (i promise i didn't look their function)\n",
        "def one_layer_book(inputs, weights, biases):\n",
        "  # Output of current layer\n",
        "  layer_outputs = []\n",
        "  # For each neuron\n",
        "  for neuron_weights, neuron_bias in zip(weights, biases):\n",
        "    # Zeroed output of given neuron\n",
        "    neuron_output = 0\n",
        "    # For each input and weight to the neuron\n",
        "    for n_input, weight in zip(inputs, neuron_weights):\n",
        "      # Multiply this input by associated weight\n",
        "      # and add to the neuron’s output variable\n",
        "      neuron_output += n_input * weight\n",
        "    # Add bias\n",
        "    neuron_output += neuron_bias\n",
        "    # Put neuron’s result to the layer’s output list\n",
        "    layer_outputs.append(neuron_output)\n",
        "\n",
        "  return layer_outputs"
      ],
      "metadata": {
        "id": "riWRFxo8uODl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 inputs, 3 neurons \n",
        "inputs  = [1.0, 2.0, 3.0, 2.5]\n",
        "\n",
        "weights1 = [0.2, 0.8, -0.5, 1.0]\n",
        "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
        "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
        "\n",
        "weights = [weights1, weights2, weights3]\n",
        "\n",
        "bias1 = 2\n",
        "bias2 = 3\n",
        "bias3 = 0.5\n",
        "\n",
        "bias = [bias1, bias2, bias3]\n",
        "\n",
        "\n",
        "print(one_layer_mine(inputs, weights, bias))\n",
        "print(one_layer_book(inputs, weights, bias))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6bmyWgshyvd",
        "outputId": "3cbe951d-40bc-4f25-d2f8-9979c25a44e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8, 1.21, 2.385]\n",
            "[4.8, 1.21, 2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensors, Arrays and Vectors"
      ],
      "metadata": {
        "id": "5vDFDwuGxeMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definition of homologous\n",
        "An homologous list/array is when each element inside the list/array has the same dimensions. An array must always be homologous, but a list can be non homologous."
      ],
      "metadata": {
        "id": "APrCnrg1hBDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# examples\n",
        "# homologous (h) list of lists (lol)\n",
        "h_lol = [[1, 2], [3, 4]]\n",
        "# non homologous (h) list of lists (lol)\n",
        "nh_lol = [[1, 2, 3], [4, 5]]\n",
        "\n",
        "# homologous array\n",
        "h_arr = np.array([[1, 2], [3, 4]])\n",
        "print(type(h_arr))\n",
        "print(h_arr)\n",
        "# non homologous array\n",
        "nh_arr = np.array([[1, 2, 3], [4, 5]])\n",
        "print(type(nh_arr))\n",
        "print(nh_arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwVyzAHfgCxn",
        "outputId": "50e39955-abaf-491f-dce3-4c62cd9a188c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "[[1 2]\n",
            " [3 4]]\n",
            "<class 'numpy.ndarray'>\n",
            "[list([1, 2, 3]) list([4, 5])]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A single neuron with numpy"
      ],
      "metadata": {
        "id": "obnEAC8nnuL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "weights = [0.2, 0.8, -0.5, 1.0]\n",
        "bias = 2.0\n",
        "\n",
        "outputs = np.dot(weights, inputs) + bias\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwXyI1wVnwy-",
        "outputId": "95c614ff-dab1-4e2c-a316-1eb56d90a9f9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A layer of neurons with numpy"
      ],
      "metadata": {
        "id": "0Or6BEvjoebc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases = [2.0, 3.0, 0.5]\n",
        "# weights is 3x4, inputs is 4, then np.dot returns a 3x1-dimensional vector\n",
        "layer_outputs = np.dot(weights, inputs) + biases\n",
        "\n",
        "print(layer_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU-nCi54oghj",
        "outputId": "845bf886-127a-4687-ed40-acd04401b828"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8   1.21  2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A batch of data\n",
        "\"\"Often, neural networks expect to take in many ​ samples​ at a time for two reasons. One reason\n",
        "is that it’s faster to train in batches in parallel processing, and the other reason is that batchesChapter 2 - Coding Our First Neurons - Neural Networks from Scratch in Python\n",
        "26\n",
        "help with generalization during training. If you fit (perform a step of a training process) on one\n",
        "sample at a time, you’re highly likely to keep fitting to that individual sample, rather than\n",
        "slowly producing general tweaks to weights and biases that fit the entire dataset. Fitting or\n",
        "training in batches gives you a higher chance of making more meaningful changes to weights\n",
        "and biases\"\" Extract from the original book"
      ],
      "metadata": {
        "id": "xYtaXKSRq0R3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Matrix product"
      ],
      "metadata": {
        "id": "_libL-2cs0jo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transposition"
      ],
      "metadata": {
        "id": "xZjRpTButx_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# np expand_dims adds a new dimension at the index of the axis\n",
        "a = [1, 2, 3]\n",
        "\n",
        "a_arr = np.array(a)\n",
        "print(a_arr.shape)\n",
        "\n",
        "a_arr = np.array([a])\n",
        "print(a_arr.shape)\n",
        "\n",
        "a_arr2 = np.expand_dims(np.array(a), axis=0)\n",
        "print(a_arr2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6yPKxcDtz7-",
        "outputId": "fa55afd7-47b1-4edb-b1e7-9966863adf38"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3,)\n",
            "(1, 3)\n",
            "(1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#transposing with numpy \n",
        "a = [1, 2, 3]\n",
        "b = [2, 3, 4]\n",
        "\n",
        "a = np.array([a])\n",
        "b = np.array([b]).T\n",
        "\n",
        "print(np.dot(a, b))\n",
        "\n",
        "# np dot performs both vector and matrix multiplication, we only have to transpose\n",
        "# for matrix multiplication"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbARxqSKvUq1",
        "outputId": "34bbb778-433a-4c01-a40d-847a74b962e3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[20]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
        "          [2.0, 5.0, -1.0, 2.0],\n",
        "          [-1.5, 2.7, 3.3, -0.8]]\n",
        "\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases = [2.0, 3.0, 0.5]\n",
        "\n",
        "outputs = np.dot(inputs, np.array(weights).T)\n",
        "print(outputs)\n",
        "outputs = outputs + biases\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx3z7ROxxWb5",
        "outputId": "7eb05586-a020-4ad1-b2f5-8ade8f001d2f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.8   -1.79   1.885]\n",
            " [ 6.9   -4.81  -0.3  ]\n",
            " [-0.59  -1.949 -0.474]]\n",
            "[[ 4.8    1.21   2.385]\n",
            " [ 8.9   -1.81   0.2  ]\n",
            " [ 1.41   1.051  0.026]]\n"
          ]
        }
      ]
    }
  ]
}